{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Gemma on Sagemaker with QLoRA\n",
    "\n",
    "- Notebook running with a different EC2 instance type\n",
    "- Use Huggingface library\n",
    "  - SQL generator Dataset from Huggingface\n",
    "  - Transformer with Gemma model\n",
    "  - SFT Trainer from TRL library by HugginFace\n",
    "  - QLoRA based training from PEFT\n",
    "  - Deploy using saved artifacts on S3\n",
    "  - evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv==1.0.1 in /opt/conda/lib/python3.11/site-packages (from -r ./requirements_local.txt (line 1)) (1.0.1)\n",
      "Collecting sagemaker==2.227.0 (from -r ./requirements_local.txt (line 2))\n",
      "  Using cached sagemaker-2.227.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting boto3==1.34.152 (from -r ./requirements_local.txt (line 3))\n",
      "  Using cached boto3-1.34.152-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting datasets==2.18.0 (from -r ./requirements_local.txt (line 4))\n",
      "  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (23.2.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (4.25.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (6.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (0.7.7)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (4.23.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (4.3.6)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (7.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (5.9.8)\n",
      "Collecting botocore<1.35.0,>=1.34.152 (from boto3==1.34.152->-r ./requirements_local.txt (line 3))\n",
      "  Using cached botocore-1.34.162-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3==1.34.152->-r ./requirements_local.txt (line 3)) (1.0.1)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3==1.34.152->-r ./requirements_local.txt (line 3))\n",
      "  Using cached s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 4)) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 4)) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.11/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 4)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 4)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 4)) (0.70.16)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->-r ./requirements_local.txt (line 4))\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 4)) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.11/site-packages (from datasets==2.18.0->-r ./requirements_local.txt (line 4)) (0.29.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.152->boto3==1.34.152->-r ./requirements_local.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==2.18.0->-r ./requirements_local.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==2.18.0->-r ./requirements_local.txt (line 4)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==2.18.0->-r ./requirements_local.txt (line 4)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==2.18.0->-r ./requirements_local.txt (line 4)) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets==2.18.0->-r ./requirements_local.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (3.21.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (2025.1.31)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from google-pasta->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (0.23.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (2025.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (1.7.6.9)\n",
      "Requirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker==2.227.0->-r ./requirements_local.txt (line 2)) (0.3.5)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets==2.18.0->-r ./requirements_local.txt (line 4)) (0.2.1)\n",
      "Using cached sagemaker-2.227.0-py3-none-any.whl (1.5 MB)\n",
      "Using cached boto3-1.34.152-py3-none-any.whl (139 kB)\n",
      "Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "Using cached botocore-1.34.162-py3-none-any.whl (12.5 MB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Using cached s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: fsspec, botocore, s3transfer, datasets, boto3, sagemaker\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.12.0\n",
      "    Uninstalling fsspec-2024.12.0:\n",
      "      Successfully uninstalled fsspec-2024.12.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.37.38\n",
      "    Uninstalling botocore-1.37.38:\n",
      "      Successfully uninstalled botocore-1.37.38\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.11.5\n",
      "    Uninstalling s3transfer-0.11.5:\n",
      "      Successfully uninstalled s3transfer-0.11.5\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.5.0\n",
      "    Uninstalling datasets-3.5.0:\n",
      "      Successfully uninstalled datasets-3.5.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.37.38\n",
      "    Uninstalling boto3-1.37.38:\n",
      "      Successfully uninstalled boto3-1.37.38\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.243.2\n",
      "    Uninstalling sagemaker-2.243.2:\n",
      "      Successfully uninstalled sagemaker-2.243.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.2 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "jupyter-ai 2.29.1 requires faiss-cpu!=1.8.0.post0,<2.0.0,>=1.8.0, which is not installed.\n",
      "aiobotocore 2.19.0 requires botocore<1.36.4,>=1.36.0, but you have botocore 1.34.162 which is incompatible.\n",
      "amazon-sagemaker-sql-magic 0.1.3 requires sqlparse==0.5.0, but you have sqlparse 0.5.3 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires nltk<3.9,>=3.4.5, but you have nltk 3.9.1 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "dask 2025.2.0 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
      "distributed 2025.2.0 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
      "langchain-aws 0.2.10 requires boto3>=1.35.74, but you have boto3 1.34.152 which is incompatible.\n",
      "sagemaker-core 1.0.22 requires boto3<2.0.0,>=1.35.75, but you have boto3 1.34.152 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.34.152 botocore-1.34.162 datasets-2.18.0 fsspec-2024.2.0 s3transfer-0.10.4 sagemaker-2.227.0\n"
     ]
    }
   ],
   "source": [
    "! pip install -r ./requirements_local.txt\n",
    "\n",
    "#Add Token as .env file for the model gemma-2b-it , pbtain permission from Higging face and create a Token in UI, copy the token as HF_TOKEN=[Your token ] in .env file#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Bucket u5-av-llmops-sagemaker-workshop got response BucketAlreadyExists\n",
      "role ='arn:aws:iam::536769277173:role/BI-DEV'\n",
      "s3_bucket ='u5-av-llmops-sagemaker-workshop'\n",
      "sess.boto_region_name ='us-west-2'\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables, SageMaker SDK, and AWS SDK\n",
    "import dotenv\n",
    "import sagemaker\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "# Useful for keeping AWS credentials, role names, or other secrets outside the code\n",
    "print(dotenv.load_dotenv('./.env'))\n",
    "\n",
    "# ---------- Configuration Section ----------\n",
    "\n",
    "# Unique user identifier to namespace resources\n",
    "USER_ID = 'u5'\n",
    "\n",
    "# Define the S3 bucket name for storing training/testing data and model artifacts\n",
    "s3_bucket = f\"{USER_ID}-av-llmops-sagemaker\"\n",
    "\n",
    "# SageMaker training job and endpoint model names\n",
    "job_name = f\"{USER_ID}-qlora-gemma-2b-sql-generator\"\n",
    "deploy_model_name = f\"{USER_ID}-sql-generator-model\"\n",
    "\n",
    "# Uncomment and set your IAM role name if needed (for local or Studio execution)\n",
    "# role_name = \"llmops_workshop_sagemaker_exec_role\"\n",
    "\n",
    "# Define the instance types for training and deployment\n",
    "train_instance = 'ml.g5.2xlarge'\n",
    "deploy_instance = 'ml.g5.2xlarge'\n",
    "\n",
    "# Base model from Hugging Face to fine-tune\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "\n",
    "# Local paths for training and test datasets and their S3 upload locations\n",
    "train_local, train_path = './tmp/train.jsonl', 'dataset/train.jsonl'\n",
    "test_local, test_path = './tmp/test.jsonl', 'dataset/test.jsonl'\n",
    "\n",
    "# ---------- S3 Bucket Setup ----------\n",
    "\n",
    "def create_bucket(bucket_name, region=\"ap-south-1\"):\n",
    "    \"\"\"\n",
    "    Create an S3 bucket in the specified region if it doesn't already exist.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3', region_name=region)\n",
    "    try:\n",
    "        location = {'LocationConstraint': region}\n",
    "        s3_client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration=location)\n",
    "        print(f\"Bucket {bucket_name} created successfully.\")\n",
    "    except ClientError as e:\n",
    "        print(f\"Bucket {bucket_name} got response {e.response['Error']['Code']}\")\n",
    "\n",
    "\n",
    "create_bucket(s3_bucket)\n",
    "\n",
    "# ---------- IAM Role Setup ----------\n",
    "\n",
    "# Attempt to get the SageMaker execution role automatically\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    # Fallback to manually fetching the role via boto3\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName=role_name)['Role']['Arn']\n",
    "\n",
    "# ---------- SageMaker Session Setup ----------\n",
    "\n",
    "# Initialize the SageMaker session with the default bucket\n",
    "sess = sagemaker.Session(default_bucket=s3_bucket)\n",
    "\n",
    "# ---------- Output Configuration Summary ----------\n",
    "\n",
    "# Print details for verification\n",
    "print(f\"{role =}\")\n",
    "print(f\"{s3_bucket =}\")\n",
    "print(f\"{sess.boto_region_name =}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 5097, 'domain': 'forestry', 'domain_description': 'Comprehensive data on sustainable forest management, timber production, wildlife habitat, and carbon sequestration in forestry.', 'sql_complexity': 'single join', 'sql_complexity_description': 'only one join (specify inner, outer, cross)', 'sql_task_type': 'analytics and reporting', 'sql_task_type_description': 'generating reports, dashboards, and analytical insights', 'sql_prompt': 'What is the total volume of timber sold by each salesperson, sorted by salesperson?', 'sql_context': \"CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\", 'sql': 'SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;', 'sql_explanation': 'Joins timber_sales and salesperson tables, groups sales by salesperson, calculates total volume sold by each salesperson, and orders the results by total volume in descending order.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"gretelai/synthetic_text_to_sql\")\n",
    "print(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (100000, 11), 'test': (5851, 11)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "        num_rows: 5851\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ds.shape)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='int32', id=None),\n",
       " 'domain': Value(dtype='string', id=None),\n",
       " 'domain_description': Value(dtype='string', id=None),\n",
       " 'sql_complexity': Value(dtype='string', id=None),\n",
       " 'sql_complexity_description': Value(dtype='string', id=None),\n",
       " 'sql_task_type': Value(dtype='string', id=None),\n",
       " 'sql_task_type_description': Value(dtype='string', id=None),\n",
       " 'sql_prompt': Value(dtype='string', id=None),\n",
       " 'sql_context': Value(dtype='string', id=None),\n",
       " 'sql': Value(dtype='string', id=None),\n",
       " 'sql_explanation': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].features # Data Set Features#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 5097\n",
      "\n",
      "\n",
      "domain: forestry\n",
      "\n",
      "\n",
      "domain_description: Comprehensive data on sustainable forest management, timber production, wildlife habitat, and carbon sequestration in forestry.\n",
      "\n",
      "\n",
      "sql_complexity: single join\n",
      "\n",
      "\n",
      "sql_complexity_description: only one join (specify inner, outer, cross)\n",
      "\n",
      "\n",
      "sql_task_type: analytics and reporting\n",
      "\n",
      "\n",
      "sql_task_type_description: generating reports, dashboards, and analytical insights\n",
      "\n",
      "\n",
      "sql_prompt: What is the total volume of timber sold by each salesperson, sorted by salesperson?\n",
      "\n",
      "\n",
      "sql_context: CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\n",
      "\n",
      "\n",
      "sql: SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\n",
      "\n",
      "\n",
      "sql_explanation: Joins timber_sales and salesperson tables, groups sales by salesperson, calculates total volume sold by each salesperson, and orders the results by total volume in descending order.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for label, val in ds[\"train\"][0].items():\n",
    "    print(f\"{label}: {val}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPT_TEMPLATE = \"\"\" \n",
    "You are a database management system expert, proficient in Structured Query Language (SQL). \n",
    "Your job is to write an SQL query that answers the following question, based on the given database schema and any additional information provided. \n",
    "Use SQLite syntax and please output only SQL without any kind of explanations. \n",
    "### Schema: {sql_context} \n",
    " \n",
    "### Knowledge: This \"{sql_task_type}\" type task is commonly used for {sql_task_type_description} in the domain of {domain}, which involves {domain_description}. \n",
    " \n",
    "### Question: {sql_prompt} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_messages(item_dict):\n",
    "    \"\"\"\n",
    "    Formats a dictionary of SQL task metadata into a message exchange format\n",
    "    suitable for chat-based fine-tuning or prompting with an LLM.\"\"\"\n",
    "    return { \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT_TEMPLATE.format(**item_dict)},\n",
    "        {\"role\": \"assistant\", \"content\": item_dict[\"sql\"]}\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': ' \\nYou are a database management system expert, proficient in Structured Query Language (SQL). \\nYour job is to write an SQL query that answers the following question, based on the given database schema and any additional information provided. \\nUse SQLite syntax and please output only SQL without any kind of explanations. \\n### Schema: CREATE TABLE equipment_maintenance (equipment_type VARCHAR(255), maintenance_frequency INT); \\n \\n### Knowledge: This \"analytics and reporting\" type task is commonly used for generating reports, dashboards, and analytical insights in the domain of defense industry, which involves Defense contract data, military equipment maintenance, threat intelligence metrics, and veteran employment stats.. \\n \\n### Question: List all the unique equipment types and their corresponding total maintenance frequency from the equipment_maintenance table. \\n',\n",
       "   'role': 'user'},\n",
       "  {'content': 'SELECT equipment_type, SUM(maintenance_frequency) AS total_maintenance_frequency FROM equipment_maintenance GROUP BY equipment_type;',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset  # Only needed if you're loading from HF Hub\n",
    "\n",
    "# Load tokenizer (ensure model_id is defined before this line)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Apply transformation to dataset\n",
    "# It’s good practice to map over each split independently (e.g., train/test)\n",
    "formated_ds = ds.map(\n",
    "    get_messages,\n",
    "    remove_columns=ds[\"train\"].column_names,  # Or use list(ds[\"train\"].features)\n",
    "    batched=False\n",
    ")\n",
    "\n",
    "# Inspect a sample record\n",
    "print(formated_ds['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nYou are a database management system expert, proficient in Structured Query Language (SQL). \\nYour job is to write an SQL query that answers the following question, based on the given database schema and any additional information provided. \\nUse SQLite syntax and please output only SQL without any kind of explanations. \\n### Schema: CREATE SCHEMA if not exists defense; CREATE TABLE if not exists eu_humanitarian_assistance (id INT PRIMARY KEY, year INT, spending INT); INSERT INTO defense.eu_humanitarian_assistance (id, year, spending) VALUES (1, 2019, 1500), (2, 2020, 1800), (3, 2021, 2100); \\n \\n### Knowledge: This \"analytics and reporting\" type task is commonly used for generating reports, dashboards, and analytical insights in the domain of defense operations, which involves Defense data on military innovation, peacekeeping operations, defense diplomacy, and humanitarian assistance.. \\n \\n### Question: What is the total spending on humanitarian assistance by the European Union in the last 3 years?<end_of_turn>\\n<start_of_turn>model\\nSELECT SUM(spending) FROM defense.eu_humanitarian_assistance WHERE year BETWEEN 2019 AND 2021;<end_of_turn>\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(formated_ds[\"train\"][5]['messages'], tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 5098,\n",
       " 'domain': 'defense industry',\n",
       " 'domain_description': 'Defense contract data, military equipment maintenance, threat intelligence metrics, and veteran employment stats.',\n",
       " 'sql_complexity': 'aggregation',\n",
       " 'sql_complexity_description': 'aggregation functions (COUNT, SUM, AVG, MIN, MAX, etc.), and HAVING clause',\n",
       " 'sql_task_type': 'analytics and reporting',\n",
       " 'sql_task_type_description': 'generating reports, dashboards, and analytical insights',\n",
       " 'sql_prompt': 'List all the unique equipment types and their corresponding total maintenance frequency from the equipment_maintenance table.',\n",
       " 'sql_context': 'CREATE TABLE equipment_maintenance (equipment_type VARCHAR(255), maintenance_frequency INT);',\n",
       " 'sql': 'SELECT equipment_type, SUM(maintenance_frequency) AS total_maintenance_frequency FROM equipment_maintenance GROUP BY equipment_type;',\n",
       " 'sql_explanation': 'This query groups the equipment_maintenance table by equipment_type and calculates the sum of maintenance_frequency for each group, then returns the equipment_type and the corresponding total_maintenance_frequency.'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Buckets: ['536769277173-sagemaker-us-west-2', 'allens-test', 'auth0-dev-events', 'auth0-email-events-dev', 'aws-glue-cdm-output', 'aws-glue-cdm-test-push', 'aws-glue-scripts-536769277173-us-west-2', 'aws-glue-temporary-536769277173-us-west-2', 'capricorn-sagemaker', 'cb-knowledgebase-test', 'cdm-d-partner-delivery-001', 'cdm-data-engg-tf-infra-backend-dev', 'cdm-data-science', 'cdm-dev-auth0-activity', 'cdm-dev-auth0-ses-activity', 'cdm-dev-chatbot-feedback', 'cdm-ds-batch-scoring', 'cdm-patch-manager-logs-536769277173', 'cdm-revjet-post-processing', 'cdm-simple-test', 'cdmtestsftp', 'cf-templates-1w94ssdr08wz4-us-west-2', 'chatbot-intent-recognition-test', 'ds-changepoint-detection-dev', 'esteban-dev', 'folders-sagemaker', 'foo-test-push', 'ga-temp-dev', 'genaiic-mlflow-artifacts-536769277173-us-west-2', 'gsc-topic-modeling', 'institutional-knowledge-agent-kb-test', 'kgoud', 'lhp-rtp-initial-match', 'manual-cdm-dev-chatbot-feedback', 'manual-cdm-dev-chatbot-interactions', 'mlops-optional', 'ql-intent-classification-dev', 'sagemaker-automated-execution-536769277173-us-west-2', 'sagemaker-project-p-2ah8dmdijjl6', 'sagemaker-project-p-jyo7ami538ok', 'sagemaker-project-p-tgccmrcrjibt', 'sagemaker-soln-ccr-js-z5770a-536769277173-us-west-2', 'sagemaker-studio-08rqyh61ajkw', 'sagemaker-studio-536769277173-gpxpf8vqvu4', 'sagemaker-studio-536769277173-ujidxdjhvy', 'sagemaker-studio-f45nvf2w795', 'sagemaker-studio-nb3e4xss46', 'sagemaker-studio-pnmb5muozd', 'sagemaker-studio-qnzsqn3eej', 'sagemaker-studio-tbovetawlj', 'sagemaker-studio-txmi5ot4mae', 'sagemaker-studio-wikst8ov00o', 'sagemaker-studio-xx6e9t1t9xf', 'sagemaker-studio-y7e23xg87gq', 'sagemaker-us-west-2-536769277173', 'sakkammadam', 'seabiscuit-sagemaker', 'test-objj', 'test-simple-drop', 'testt45', 'trending-cre']\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "buckets = [bucket['Name'] for bucket in s3.list_buckets()['Buckets']]\n",
    "print(\"Available Buckets:\", buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f881a25a8164525a265cbef9586655f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3089477dd9fb4ed9973bf1aa5a955ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "121946"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# pandas orient=‘records’ for jsonl. List like [{column -> value}, … , {column -> value}]\n",
    "formated_ds[\"train\"].shuffle().select(range(1000)).to_json(train_local, orient=\"records\")\n",
    "formated_ds[\"test\"].shuffle().select(range(100)).to_json(test_local, orient=\"records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-west-2\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "print(boto3.session.Session().region_name)\n",
    "s3=boto3.client('s3')\n",
    "s3.upload_file(train_local, s3_bucket, train_path)\n",
    "s3.upload_file(test_local, s3_bucket, test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "  # Dataset path for the trainer script to access input data\n",
    "  'dataset_path': '/opt/ml/input/data/training/train.jsonl',\n",
    "\n",
    "  # Base model (e.g., \"google/gemma-2b-it\")\n",
    "  'model_id': model_id,\n",
    "\n",
    "  # Tokenization and model input control\n",
    "  'max_seq_len': 3072,  # Good for models like Gemma (optimize based on GPU RAM)\n",
    "\n",
    "  # Enable QLoRA-style adapter-based fine-tuning\n",
    "  'use_qlora': True,\n",
    "\n",
    "  # Training loop parameters\n",
    "  'num_train_epochs': 1,\n",
    "  'per_device_train_batch_size': 1,\n",
    "  'gradient_accumulation_steps': 4,  # 1x4 => effective batch size = 4\n",
    "  'gradient_checkpointing': True,  # Saves memory for large models\n",
    "\n",
    "  # Optimizer\n",
    "  'optim': \"adamw_torch_fused\",  # NVIDIA fused optim (needs PyTorch 2.x)\n",
    "\n",
    "  # Logging\n",
    "  'logging_steps': 5,\n",
    "\n",
    "  # Checkpointing\n",
    "  'save_strategy': \"epoch\",\n",
    "\n",
    "  # Learning rate and scheduler\n",
    "  'learning_rate': 2e-4,\n",
    "  'lr_scheduler_type': \"constant\",\n",
    "  'warmup_ratio': 0.03,\n",
    "\n",
    "  # Numerical precision settings\n",
    "  'bf16': True,   # Use BF16 if your GPU supports it (like A100, H100)\n",
    "  'tf32': True,   # Use TF32 for matrix multiplies on Ampere GPUs (NVIDIA)\n",
    "\n",
    "  # Gradient control\n",
    "  'max_grad_norm': 0.3,\n",
    "\n",
    "  # Output logging and artifacts\n",
    "  'report_to': \"tensorboard\",        # Optional: \"wandb\" or None\n",
    "  'output_dir': '/tmp/tun',\n",
    "\n",
    "  # Post-training: merge LoRA adapters into base model weights\n",
    "  'merge_adapters': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# image_uri = \"763104351884.dkr.ecr.ap-south-1.amazonaws.com/huggingface-pytorch-training:2.1.0-transformers4.36.0-gpu-py310-cu121-ubuntu20.04\"\n",
    "huggingface_estimator = HuggingFace(\n",
    "     base_job_name=job_name,\n",
    "    # image_uri=image_uri,\n",
    "    # if not image_uri\n",
    "    transformers_version = '4.36.0',\n",
    "    pytorch_version      = '2.1.0',\n",
    "    \n",
    "    instance_type=train_instance,\n",
    "    instance_count=1,\n",
    "    max_run=int(3600 * 0.5),\n",
    "    role=role,\n",
    "    environment={\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\",\n",
    "        \"HF_TOKEN\": os.environ[\"HF_TOKEN\"]\n",
    "    },\n",
    "    py_version='py310',\n",
    "    entry_point='qlora.py',\n",
    "    source_dir=\".\", # Copy source to S3 and auto installs the requirements.txt file \n",
    "    hyperparameters=hyperparameters,\n",
    "    disable_output_compression = True, # not compress output to save training time and cost\n",
    "    metric_definitions=[\n",
    "      {'Name': 'loss', 'Regex': \"'loss': (.*?),\"},\n",
    "      {'Name': 'grad_norm', 'Regex': \"'grad_norm': (.*?),\"},\n",
    "      {'Name': 'learning_rate', 'Regex': \"'learning_rate': (.*?),\"},\n",
    "      {'Name': 'epoch', 'Regex': \"'epoch': (.*?)}\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/21/25 20:27:42] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> SageMaker Python SDK will collect telemetry to help us better  <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/telemetry/telemetry_logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">telemetry_logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/telemetry/telemetry_logging.py#91\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">91</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         understand our user's needs, diagnose issues, and deliver      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         additional features.                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To opt out of telemetry, please disable via TelemetryOptOut    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         parameter in SDK defaults config. For more information, refer  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         to                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://sagemaker.readthedocs.io/en/stable/overview.html#confi</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">guring-and-using-defaults-with-the-sagemaker-python-sdk.</span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/21/25 20:27:42]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m SageMaker Python SDK will collect telemetry to help us better  \u001b]8;id=459455;file:///opt/conda/lib/python3.11/site-packages/sagemaker/telemetry/telemetry_logging.py\u001b\\\u001b[2mtelemetry_logging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=677327;file:///opt/conda/lib/python3.11/site-packages/sagemaker/telemetry/telemetry_logging.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         understand our user's needs, diagnose issues, and deliver      \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         additional features.                                           \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To opt out of telemetry, please disable via TelemetryOptOut    \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         parameter in SDK defaults config. For more information, refer  \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         to                                                             \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://sagemaker.readthedocs.io/en/stable/overview.html#confi\u001b[0m \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mguring-and-using-defaults-with-the-sagemaker-python-sdk.\u001b[0m       \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> image_uri is not presented, retrieving image_uri based on            <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#681\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">681</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         instance_type, framework etc.                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m image_uri is not presented, retrieving image_uri based on            \u001b]8;id=840819;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=275440;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#681\u001b\\\u001b[2m681\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         instance_type, framework etc.                                        \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating training-job with name:                                       <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#1042\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1042</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         u5-qlora-gemma-2b-sql-generator-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-04-21-20-27-42-011                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating training-job with name:                                       \u001b]8;id=486740;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=78361;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#1042\u001b\\\u001b[2m1042\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         u5-qlora-gemma-2b-sql-generator-\u001b[1;36m2025\u001b[0m-04-21-20-27-42-011                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 20:27:43 Starting - Starting the training job\n",
      "...........20:27:43 Pending - Training job waiting for capacity.\n",
      "..25-04-21 20:29:44 Pending - Preparing the instances for training.\n",
      "..25-04-21 20:30:15 Downloading - Downloading input data.\n",
      ".................30 Downloading - Downloading the training image.\n",
      ".\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m..\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2025-04-21 20:34:00,845 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-04-21 20:34:00,862 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-04-21 20:34:00,871 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-04-21 20:34:00,873 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-04-21 20:34:02,375 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.38.2 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets==2.18.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.18.0)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.27.2 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate==0.4.1 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.42.0 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl==0.7.11 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.7.11-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.8.2 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.8.2-py3-none-any.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting flash-attn==2.5.6 (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading flash_attn-2.5.6.tar.gz (2.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 70.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (3.15.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (0.25.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (2024.9.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (2.32.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (0.15.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (0.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (4.66.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (17.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->-r requirements.txt (line 2)) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.10.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2->-r requirements.txt (line 3)) (6.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2->-r requirements.txt (line 3)) (2.1.0)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate==0.4.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.42.0->-r requirements.txt (line 5)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.7.11->-r requirements.txt (line 6)) (0.8.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.5.6->-r requirements.txt (line 8)) (0.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.5.6->-r requirements.txt (line 8)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (2.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (23.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (6.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2->-r requirements.txt (line 1)) (4.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (1.26.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (2024.7.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (1.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (3.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (0.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (13.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (2.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (0.1.2)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/8.5 MB 133.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.27.2-py3-none-any.whl (279 kB)\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.0/105.0 MB 121.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.7.11-py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mDownloading peft-0.8.2-py3-none-any.whl (183 kB)\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.5.6-cp310-cp310-linux_x86_64.whl size=120352136 sha256=63ab5a3883b67719671e154beb91522e2901cbe4af0c5e031306a06a85df0be5\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/a8/1c/88/b959d6818b98a46d61ba231683abb7523b89ac1a7ed1e0c206\u001b[0m\n",
      "\u001b[34mSuccessfully built flash-attn\u001b[0m\n",
      "\u001b[34mInstalling collected packages: responses, bitsandbytes, flash-attn, accelerate, transformers, peft, trl, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: bitsandbytes\u001b[0m\n",
      "\u001b[34mFound existing installation: bitsandbytes 0.44.1\u001b[0m\n",
      "\u001b[34mUninstalling bitsandbytes-0.44.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled bitsandbytes-0.44.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 2.3.6\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-2.3.6:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-2.3.6\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.26.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.26.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.36.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.36.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.36.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: peft\u001b[0m\n",
      "\u001b[34mFound existing installation: peft 0.7.1\u001b[0m\n",
      "\u001b[34mUninstalling peft-0.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled peft-0.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: trl\u001b[0m\n",
      "\u001b[34mFound existing installation: trl 0.7.4\u001b[0m\n",
      "\u001b[34mUninstalling trl-0.7.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled trl-0.7.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: evaluate\u001b[0m\n",
      "\u001b[34mFound existing installation: evaluate 0.4.3\u001b[0m\n",
      "\u001b[34mUninstalling evaluate-0.4.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled evaluate-0.4.3\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.27.2 bitsandbytes-0.42.0 evaluate-0.4.1 flash-attn-2.5.6 peft-0.8.2 responses-0.18.0 transformers-4.38.2 trl-0.7.11\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.2 -> 25.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2025-04-21 20:34:22,745 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-04-21 20:34:22,745 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-04-21 20:34:22,787 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-04-21 20:34:22,815 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-04-21 20:34:22,845 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-04-21 20:34:22,856 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training/train.jsonl\",\n",
      "        \"gradient_accumulation_steps\": 4,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 5,\n",
      "        \"lr_scheduler_type\": \"constant\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"max_seq_len\": 3072,\n",
      "        \"merge_adapters\": true,\n",
      "        \"model_id\": \"google/gemma-2b-it\",\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"optim\": \"adamw_torch_fused\",\n",
      "        \"output_dir\": \"/tmp/tun\",\n",
      "        \"per_device_train_batch_size\": 1,\n",
      "        \"report_to\": \"tensorboard\",\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"tf32\": true,\n",
      "        \"use_qlora\": true,\n",
      "        \"warmup_ratio\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"u5-qlora-gemma-2b-sql-generator-2025-04-21-20-27-42-011\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-536769277173/u5-qlora-gemma-2b-sql-generator-2025-04-21-20-27-42-011/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"qlora\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"qlora.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training/train.jsonl\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":5,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_len\":3072,\"merge_adapters\":true,\"model_id\":\"google/gemma-2b-it\",\"num_train_epochs\":1,\"optim\":\"adamw_torch_fused\",\"output_dir\":\"/tmp/tun\",\"per_device_train_batch_size\":1,\"report_to\":\"tensorboard\",\"save_strategy\":\"epoch\",\"tf32\":true,\"use_qlora\":true,\"warmup_ratio\":0.03}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=qlora.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=qlora\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-536769277173/u5-qlora-gemma-2b-sql-generator-2025-04-21-20-27-42-011/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training/train.jsonl\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":5,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_len\":3072,\"merge_adapters\":true,\"model_id\":\"google/gemma-2b-it\",\"num_train_epochs\":1,\"optim\":\"adamw_torch_fused\",\"output_dir\":\"/tmp/tun\",\"per_device_train_batch_size\":1,\"report_to\":\"tensorboard\",\"save_strategy\":\"epoch\",\"tf32\":true,\"use_qlora\":true,\"warmup_ratio\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"u5-qlora-gemma-2b-sql-generator-2025-04-21-20-27-42-011\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-536769277173/u5-qlora-gemma-2b-sql-generator-2025-04-21-20-27-42-011/source/sourcedir.tar.gz\",\"module_name\":\"qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"qlora.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training/train.jsonl\",\"--gradient_accumulation_steps\",\"4\",\"--gradient_checkpointing\",\"True\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"5\",\"--lr_scheduler_type\",\"constant\",\"--max_grad_norm\",\"0.3\",\"--max_seq_len\",\"3072\",\"--merge_adapters\",\"True\",\"--model_id\",\"google/gemma-2b-it\",\"--num_train_epochs\",\"1\",\"--optim\",\"adamw_torch_fused\",\"--output_dir\",\"/tmp/tun\",\"--per_device_train_batch_size\",\"1\",\"--report_to\",\"tensorboard\",\"--save_strategy\",\"epoch\",\"--tf32\",\"True\",\"--use_qlora\",\"True\",\"--warmup_ratio\",\"0.03\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training/train.jsonl\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=4\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=5\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=constant\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LEN=3072\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_ADAPTERS=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=google/gemma-2b-it\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIM=adamw_torch_fused\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/tmp/tun\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_REPORT_TO=tensorboard\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_TF32=true\u001b[0m\n",
      "\u001b[34mSM_HP_USE_QLORA=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.03\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 qlora.py --bf16 True --dataset_path /opt/ml/input/data/training/train.jsonl --gradient_accumulation_steps 4 --gradient_checkpointing True --learning_rate 0.0002 --logging_steps 5 --lr_scheduler_type constant --max_grad_norm 0.3 --max_seq_len 3072 --merge_adapters True --model_id google/gemma-2b-it --num_train_epochs 1 --optim adamw_torch_fused --output_dir /tmp/tun --per_device_train_batch_size 1 --report_to tensorboard --save_strategy epoch --tf32 True --use_qlora True --warmup_ratio 0.03\u001b[0m\n",
      "\u001b[34m2025-04-21 20:34:22,858 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2025-04-21 20:34:22,858 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1000 examples [00:00, 154680.04 examples/s]\u001b[0m\n",
      "\u001b[34mUsing QLoRA\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:16<00:16, 16.02s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:16<00:00,  6.87s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:16<00:00,  8.24s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.85s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1 examples [00:00,  5.15 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 90 examples [00:00, 363.54 examples/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/22 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34m5%|▍         | 1/22 [00:07<02:46,  7.91s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 2/22 [00:15<02:36,  7.81s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 3/22 [00:23<02:27,  7.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 4/22 [00:31<02:19,  7.77s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 5/22 [00:38<02:11,  7.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9438, 'grad_norm': 0.82421875, 'learning_rate': 0.0002, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 5/22 [00:38<02:11,  7.76s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 6/22 [00:46<02:04,  7.76s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 7/22 [00:54<01:56,  7.75s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 8/22 [01:02<01:48,  7.75s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 9/22 [01:09<01:40,  7.75s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 10/22 [01:17<01:33,  7.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2946, 'grad_norm': 0.73046875, 'learning_rate': 0.0002, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 10/22 [01:17<01:33,  7.75s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 11/22 [01:25<01:25,  7.75s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 12/22 [01:33<01:17,  7.75s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 13/22 [01:40<01:09,  7.75s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 14/22 [01:48<01:02,  7.78s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 15/22 [01:56<00:54,  7.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0824, 'grad_norm': 0.87890625, 'learning_rate': 0.0002, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 15/22 [01:56<00:54,  7.77s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 16/22 [02:04<00:46,  7.77s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 17/22 [02:11<00:38,  7.76s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 18/22 [02:19<00:31,  7.76s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 19/22 [02:27<00:23,  7.75s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 20/22 [02:35<00:15,  7.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9003, 'grad_norm': 2.28125, 'learning_rate': 0.0002, 'epoch': 0.89}\u001b[0m\n",
      "\u001b[34m91%|█████████ | 20/22 [02:35<00:15,  7.75s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 21/22 [02:42<00:07,  7.75s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 22/22 [02:50<00:00,  7.75s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 171.9874, 'train_samples_per_second': 0.523, 'train_steps_per_second': 0.128, 'train_loss': 1.2661530808968977, 'epoch': 0.98}\u001b[0m\n",
      "\u001b[34m100%|██████████| 22/22 [02:51<00:00,  7.75s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 22/22 [02:51<00:00,  7.82s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m['tokenizer.model', 'checkpoint-22', 'README.md', 'tokenizer.json', 'adapter_model.safetensors', 'runs', 'special_tokens_map.json', 'tokenizer_config.json', 'adapter_config.json']\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.64s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\u001b[0m\n",
      "\u001b[34m2025-04-21 20:37:59,807 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-04-21 20:37:59,808 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-04-21 20:37:59,808 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-04-21 20:38:03 Uploading - Uploading generated training model\n",
      "2025-04-21 20:38:31 Completed - Training job completed\n",
      "Training seconds: 496\n",
      "Billable seconds: 496\n"
     ]
    }
   ],
   "source": [
    "data = {'training': f's3://{s3_bucket}/dataset'}\n",
    "huggingface_estimator.fit(data, wait=True) # Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model and Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "#S3 path to the model artifacts from training\n",
    "\n",
    "model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "\n",
    "# Container URI for Hugging Face Text Generation Inference (TGI)\n",
    "# Make sure region matches where you'll deploy the model\n",
    " \n",
    "llm_image = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.2.0-gpu-py310-cu121-ubuntu22.04-v2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    " \n",
    "# Environment variables for TGI container\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\",\n",
    "  'SM_NUM_GPUS': '1',\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024),\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048) # req prompt tokens + req generated tokens in the GPU for this req\n",
    "}\n",
    "# HuggingFaceModel expects `model_data` to be a string (S3 URI), not a dictionar\n",
    "llm_model = HuggingFaceModel(\n",
    "  name=deploy_model_name,\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  model_data={'S3DataSource':{'S3Uri': model_s3_path,'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/21/25 21:22:14] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name: u5-sql-generator-model                       <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4094</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/21/25 21:22:14]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name: u5-sql-generator-model                       \u001b]8;id=347841;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=119251;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\u001b\\\u001b[2m4094\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/21/25 21:22:15] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint-config with name                                     <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#6019\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6019</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         u5-sql-generator-model-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-04-21-21-22-15-729                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/21/25 21:22:15]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint-config with name                                     \u001b]8;id=946147;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=63029;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#6019\u001b\\\u001b[2m6019\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         u5-sql-generator-model-\u001b[1;36m2025\u001b[0m-04-21-21-22-15-729                         \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/21/25 21:22:16] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint with name                                            <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4841\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4841</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         u5-sql-generator-model-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-04-21-21-22-15-729                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/21/25 21:22:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint with name                                            \u001b]8;id=195572;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=242874;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4841\u001b\\\u001b[2m4841\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         u5-sql-generator-model-\u001b[1;36m2025\u001b[0m-04-21-21-22-15-729                         \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=deploy_instance,\n",
    "  container_startup_health_check_timeout=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c723f8fabd4871aff975aa50a6772b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input message:\n",
      " {'content': ' \\nYou are a database management system expert, proficient in Structured Query Language (SQL). \\nYour job is to write an SQL query that answers the following question, based on the given database schema and any additional information provided. \\nUse SQLite syntax and please output only SQL without any kind of explanations. \\n### Schema: CREATE TABLE costs (county_id INT, year INT, cost INT); \\n \\n### Knowledge: This \"analytics and reporting\" type task is commonly used for generating reports, dashboards, and analytical insights in the domain of rural health, which involves Detailed records on healthcare access, disease prevalence, and resource allocation in rural health.. \\n \\n### Question: Identify counties in New Mexico with increasing healthcare costs over the past 4 years. \\n', 'role': 'user'} \n",
      "\n",
      "\n",
      "expected output:\n",
      " {'content': \"SELECT county_id, COUNT(*) AS years FROM costs WHERE costs[ROW_NUMBER() OVER (PARTITION BY county_id ORDER BY year) - 1] < cost GROUP BY county_id HAVING COUNT(*) = 4 AND county_id IN (SELECT county_id FROM costs WHERE state = 'New Mexico');\", 'role': 'assistant'} \n",
      "\n",
      "\n",
      "prompt:\n",
      " <bos><start_of_turn>user\n",
      "You are a database management system expert, proficient in Structured Query Language (SQL). \n",
      "Your job is to write an SQL query that answers the following question, based on the given database schema and any additional information provided. \n",
      "Use SQLite syntax and please output only SQL without any kind of explanations. \n",
      "### Schema: CREATE TABLE costs (county_id INT, year INT, cost INT); \n",
      " \n",
      "### Knowledge: This \"analytics and reporting\" type task is commonly used for generating reports, dashboards, and analytical insights in the domain of rural health, which involves Detailed records on healthcare access, disease prevalence, and resource allocation in rural health.. \n",
      " \n",
      "### Question: Identify counties in New Mexico with increasing healthcare costs over the past 4 years.<end_of_turn>\n",
      "<start_of_turn>model\n",
      " \n",
      "\n",
      "\n",
      "generated output:\n",
      " {'role': 'assistant', 'content': 'SELECT county_id, year FROM costs WHERE county_id IN (SELECT county_id FROM costs WHERE year > 2020) AND cost > (SELECT cost FROM costs WHERE county_id = (SELECT county_id FROM costs WHERE year > 2016) AND year <= 2020);Життєпис_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo_output; vedo'} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from datasets import load_dataset\n",
    "# ---------- Configuration ----------\n",
    "os.makedirs('./tmp', exist_ok=True)\n",
    "# Ensure model_id, s3_bucket, and test_path are defined earlier in your script\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Download test dataset from S3\n",
    "boto3.client('s3').download_file(s3_bucket, test_path, test_local)\n",
    "# Load dataset using Hugging Face Datasets\n",
    "test_dataset = load_dataset(\"json\", data_files=test_local, split=\"train\")\n",
    "\n",
    "#  Make sure `llm.endpoint_name` or the actual endpoint name is defined\n",
    "deployed_llm = Predictor(\n",
    "    endpoint_name=llm.endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")\n",
    "# ---------- Inference Function ----------\n",
    "def request(sample):\n",
    "    prompt = tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=True)\n",
    "    print(f\"prompt:\\n {prompt} \\n\\n\")\n",
    "    outputs = deployed_llm.predict({\n",
    "      \"inputs\": prompt,\n",
    "      \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"temperature\": 0.01,\n",
    "        \"return_full_text\": False,\n",
    "      }\n",
    "    })\n",
    "    return {\"role\": \"assistant\", \"content\": outputs[0][\"generated_text\"].strip()}\n",
    "\n",
    "random_sample = test_dataset[10]\n",
    "print(f\"input message:\\n {random_sample['messages'][0]} \\n\\n\")\n",
    "print(f\"expected output:\\n {random_sample['messages'][1]} \\n\\n\")\n",
    "print(f\"generated output:\\n {request([random_sample['messages'][0]])} \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/21/25 22:41:21] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting model with name: u5-sql-generator-model                       <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#5356\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5356</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/21/25 22:41:21]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting model with name: u5-sql-generator-model                       \u001b]8;id=124834;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=655730;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#5356\u001b\\\u001b[2m5356\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting endpoint configuration with name:                             <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4995\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4995</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         u5-sql-generator-model-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-04-21-21-22-15-729                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting endpoint configuration with name:                             \u001b]8;id=90171;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=724316;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4995\u001b\\\u001b[2m4995\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         u5-sql-generator-model-\u001b[1;36m2025\u001b[0m-04-21-21-22-15-729                         \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/21/25 22:41:22] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting endpoint with name:                                           <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4985\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4985</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         u5-sql-generator-model-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-04-21-21-22-15-729                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/21/25 22:41:22]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting endpoint with name:                                           \u001b]8;id=929913;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=767022;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4985\u001b\\\u001b[2m4985\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         u5-sql-generator-model-\u001b[1;36m2025\u001b[0m-04-21-21-22-15-729                         \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#Deleting the end points\n",
    "deployed_llm.delete_model()\n",
    "deployed_llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
